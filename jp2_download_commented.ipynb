{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "os.chdir(r\"C:\\Users\\rdalwadi\\Documents\\lawpdfs\")\n",
    "\n",
    "with open(\"search.csv\",\"r\") as identifiers:\n",
    "# Opens the file with identifiers for parsing\n",
    "\n",
    "    reader=csv.DictReader(identifiers)\n",
    "    l=[d['identifier'] for d in reader]\n",
    "# print(l[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=0\n",
    "start=time.time()\n",
    "problems=list()\n",
    "fails=0\n",
    "sourcelink = 'https://archive.org/download/'  # A single web source contains all the files to download\n",
    "for f in l:\n",
    "# The identifiers are used to generate links to the images for download\n",
    "\n",
    "    try:\n",
    "        ct+=1\n",
    "        # A download link is created for each file by appending the id and file extension to the source\n",
    "        urllib.request.urlretrieve(full_link, f+'_jp2.zip')\n",
    "        full_link = sourcelink+f+'/'+f+'_jp2.zip'\n",
    "        time.sleep(120)\n",
    "        if ct%10==0:\n",
    "            print(str(ct)+\": \"+f)\n",
    "            print(time.time()-start)\n",
    "    except:\n",
    "        fails+=1\n",
    "        time.sleep(60)\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for and resolving problems:\n",
    "for f in l:\n",
    "# Get a list of the missing folders\n",
    "    zips= sourcelink + f +'/' +f+ '_jp2.zip' \n",
    "missed=[f for f in zips if f.split(\"/\")[-1] not in os.listdir(\".\")]\n",
    "print(missed)\n",
    "#manually download\n",
    "\n",
    "broken_dl=[]\n",
    "\n",
    "for z in zips:\n",
    "# Get a list of the items with broken links by comparing file sizes of the\n",
    "# original file to the downloaded file\n",
    "\n",
    "    local=z.split(\"/\")[-1]\n",
    "    local_size=os.path.getsize(local)\n",
    "    with urllib.request.urlopen(z) as web:\n",
    "        meta=dict(web.info())\n",
    "        web_size=int(meta[\"Content-Length\"])\n",
    "    if web_size!=local_size:\n",
    "        broken_dl.append(z)\n",
    "\n",
    "for z in broken_dl:\n",
    "# Print the sizes of the downloaded and original files\n",
    "\n",
    "    local=z.split(\"/\")[-1]\n",
    "    local_size=os.path.getsize(local)\n",
    "    with urllib.request.urlopen(z) as web:\n",
    "        meta=dict(web.info())\n",
    "        web_size=int(meta[\"Content-Length\"])\n",
    "    print(local_size/(1024^2),web_size/(1024^2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
